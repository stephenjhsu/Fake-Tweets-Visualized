{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Russian Bot Discussion\n",
    "\n",
    "The name of our project is: Russian Bot Discussion\n",
    "    \n",
    "Email Addresses: David Kes: (ddkes@dons.usfca.edu) Stephen Hsu: (sjhsu@dons.usfca.edu)\n",
    "            \n",
    "Link to project: https://github.com/stephenjhsu/msan622viz\n",
    "\n",
    "## Background and Motivation\n",
    "\n",
    "With the fervor of the Presidential election being skewed by Russian probing’s as well as the notorious Facebook / Cambridge Analytics scandal still topping daily domestic headlines, it became clear to us that “fake news” and Russian bots are still prevalent, yet vague concepts. What exactly are these bots saying? How are people being fooled by these Tweets? How exactly are they influencing people and spreading propaganda? Therefore, with our backgrounds in natural language processing, data visualization, and interest in the combination of technology and politics, it was only natural to examine the Russian bot Tweet data with Python and Plotly. The following is our process of using NBC data found at https://www.nbcnews.com/tech/social-media/now-available-more-200-000-deleted-russian-troll-tweets-n844731 and visualizing them for topical modeling, sentiment analysis, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#nlp\n",
    "import spacy\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n",
    "\n",
    "#LDA / topical modeling\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, fldname, drop=True, time=True):\n",
    "    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n",
    "    the information from the date. This applies changes inplace.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame. df gain several new columns.\n",
    "    fldname: A string that is the name of the date column you wish to expand.\n",
    "        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n",
    "    drop: If true then the original date column will be removed.\n",
    "    time: If true time features: Hour, Minute, Second will be added.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })\n",
    "    >>> df\n",
    "        A\n",
    "    0   2000-03-11\n",
    "    1   2000-03-12\n",
    "    2   2000-03-13\n",
    "    >>> add_datepart(df, 'A')\n",
    "    >>> df\n",
    "        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n",
    "    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n",
    "    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n",
    "    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n",
    "    \"\"\"\n",
    "    fld = df[fldname]\n",
    "    if not np.issubdtype(fld.dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Day', 'Dayofweek', 'Dayofyear']\n",
    "    if time: attr = attr + ['Hour', 'Minute']\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "tweets = pd.read_csv('../../finalprojdata/tweets.csv')\n",
    "users = pd.read_csv('../../finalprojdata/users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop(['tweet_id','retweeted_status_id', 'in_reply_to_status_id', 'created_at', 'expanded_urls'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltweets = tweets.merge(users, how='left', left_on='user_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltweets = fulltweets[pd.notnull(fulltweets['user_id'])]\n",
    "fulltweets = fulltweets[pd.notnull(fulltweets['created_str'])]\n",
    "fulltweets = fulltweets[pd.notnull(fulltweets['friends_count'])]\n",
    "fulltweets = fulltweets[pd.notnull(fulltweets['time_zone'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185160"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fulltweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                  0\n",
       "user_key                 0\n",
       "created_str              0\n",
       "retweet_count       134149\n",
       "retweeted           134149\n",
       "favorite_count      134149\n",
       "text                     0\n",
       "source              134149\n",
       "hashtags                 0\n",
       "posted                   0\n",
       "mentions                 0\n",
       "id                       0\n",
       "location             20155\n",
       "name                     0\n",
       "followers_count          0\n",
       "statuses_count           0\n",
       "time_zone                0\n",
       "verified                 0\n",
       "lang                     0\n",
       "screen_name              0\n",
       "description          11813\n",
       "created_at               0\n",
       "favourites_count         0\n",
       "friends_count            0\n",
       "listed_count             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulltweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix time\n",
    "add_datepart(fulltweets, 'created_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sentiment column\n",
    "def analyze_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    analysis = TextBlob(tweet)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a subjectivity column\n",
    "def analyze_sentiment2(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    analysis = TextBlob(tweet)\n",
    "    return analysis.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsonly2 = fulltweets.text.copy().astype(str)\n",
    "tweetsonly2 = tweetsonly2.str.replace('[^\\w\\s]','')\n",
    "tweetsonly2 = tweetsonly2.str.replace('[\\\\r|\\\\n|\\\\t|_]',' ')\n",
    "tweetsonly2 = tweetsonly2.str.strip()\n",
    "\n",
    "fulltweets2 = fulltweets.copy()\n",
    "fulltweets2.text = tweetsonly2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop += ['rt']\n",
    "fulltweets2.text = fulltweets2.text.apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop)]))\n",
    "fulltweets2['Sentiment'] = np.array([analyze_sentiment(str(tweet)) for tweet in fulltweets2.text.values])\n",
    "fulltweets2['Polarity'] = np.array([analyze_sentiment2(str(tweet)) for tweet in fulltweets2.text.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltweets2.text = fulltweets2.text.apply(lambda x: ' '.join([word.lower() for word in x.split() if len(word) > 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltweets2.to_csv('/Users/shsu/Downloads/fulltweets2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
