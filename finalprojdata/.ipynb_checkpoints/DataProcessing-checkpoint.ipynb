{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#nlp\n",
    "import spacy\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n",
    "\n",
    "#LDA / topical modeling\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "#Visualizations\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, fldname, drop=True, time=False):\n",
    "    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n",
    "    the information from the date. This applies changes inplace.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame. df gain several new columns.\n",
    "    fldname: A string that is the name of the date column you wish to expand.\n",
    "        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n",
    "    drop: If true then the original date column will be removed.\n",
    "    time: If true time features: Hour, Minute, Second will be added.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })\n",
    "    >>> df\n",
    "        A\n",
    "    0   2000-03-11\n",
    "    1   2000-03-12\n",
    "    2   2000-03-13\n",
    "    >>> add_datepart(df, 'A')\n",
    "    >>> df\n",
    "        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n",
    "    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n",
    "    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n",
    "    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n",
    "    \"\"\"\n",
    "    fld = df[fldname]\n",
    "    if not np.issubdtype(fld.dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets.csv')\n",
    "users = pd.read_csv('users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_key</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_str</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>posted</th>\n",
       "      <th>mentions</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.868981e+09</td>\n",
       "      <td>ryanmaxwell_1</td>\n",
       "      <td>1.458672e+12</td>\n",
       "      <td>2016-03-22 18:31:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>7.123460e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"IslamKills\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.571870e+09</td>\n",
       "      <td>detroitdailynew</td>\n",
       "      <td>1.476133e+12</td>\n",
       "      <td>2016-10-10 20:57:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>7.855849e+17</td>\n",
       "      <td>&lt;a href=\"http://twitterfeed.com\" rel=\"nofollow...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"http://detne.ws/2e172jF\"]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.710805e+09</td>\n",
       "      <td>cookncooks</td>\n",
       "      <td>1.487767e+12</td>\n",
       "      <td>2017-02-22 12:43:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>8.343832e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id         user_key    created_at          created_str  \\\n",
       "0  1.868981e+09    ryanmaxwell_1  1.458672e+12  2016-03-22 18:31:42   \n",
       "1  2.571870e+09  detroitdailynew  1.476133e+12  2016-10-10 20:57:00   \n",
       "2  1.710805e+09       cookncooks  1.487767e+12  2017-02-22 12:43:43   \n",
       "\n",
       "   retweet_count retweeted  favorite_count  \\\n",
       "0            NaN       NaN             NaN   \n",
       "1            0.0     False             0.0   \n",
       "2            NaN       NaN             NaN   \n",
       "\n",
       "                                                text      tweet_id  \\\n",
       "0  #IslamKills Are you trying to say that there w...  7.123460e+17   \n",
       "1  Clinton: Trump should’ve apologized more, atta...  7.855849e+17   \n",
       "2  RT @ltapoll: Who was/is the best president of ...  8.343832e+17   \n",
       "\n",
       "                                              source        hashtags  \\\n",
       "0                                                NaN  [\"IslamKills\"]   \n",
       "1  <a href=\"http://twitterfeed.com\" rel=\"nofollow...              []   \n",
       "2                                                NaN              []   \n",
       "\n",
       "                 expanded_urls  posted mentions  retweeted_status_id  \\\n",
       "0                           []  POSTED       []                  NaN   \n",
       "1  [\"http://detne.ws/2e172jF\"]  POSTED       []                  NaN   \n",
       "2                           []  POSTED       []                  NaN   \n",
       "\n",
       "   in_reply_to_status_id  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>verified</th>\n",
       "      <th>lang</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>listed_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18710816.0</td>\n",
       "      <td>near Utah Ave &amp; Lighthouse an</td>\n",
       "      <td>Robby Delaware</td>\n",
       "      <td>304.0</td>\n",
       "      <td>11484.0</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>RobbyDelaware</td>\n",
       "      <td>I support the free movement of people, ideas a...</td>\n",
       "      <td>Wed Jan 07 04:38:02 +0000 2009</td>\n",
       "      <td>17.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100345056.0</td>\n",
       "      <td>still ⬆️Block⤵️Corner⬇️street</td>\n",
       "      <td>#Ezekiel2517✨...</td>\n",
       "      <td>1053.0</td>\n",
       "      <td>31858.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>SCOTTGOHARD</td>\n",
       "      <td>CELEBRITY TRAINER ✨#424W147th✨ #CrossfitCoach ...</td>\n",
       "      <td>Tue Dec 29 23:15:22 +0000 2009</td>\n",
       "      <td>2774.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>247165706.0</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>B E C K S T E R✨</td>\n",
       "      <td>650.0</td>\n",
       "      <td>6742.0</td>\n",
       "      <td>Mountain Time (US &amp; Canada)</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Beckster319</td>\n",
       "      <td>Rebecca Lynn Hirschfeld Actress.Model.Writer.A...</td>\n",
       "      <td>Fri Feb 04 06:38:45 +0000 2011</td>\n",
       "      <td>7273.0</td>\n",
       "      <td>896.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                       location              name  \\\n",
       "0   18710816.0  near Utah Ave & Lighthouse an    Robby Delaware   \n",
       "1  100345056.0  still ⬆️Block⤵️Corner⬇️street  #Ezekiel2517✨...   \n",
       "2  247165706.0                    Chicago, IL  B E C K S T E R✨   \n",
       "\n",
       "   followers_count  statuses_count                    time_zone verified lang  \\\n",
       "0            304.0         11484.0   Pacific Time (US & Canada)    False   en   \n",
       "1           1053.0         31858.0                          NaN    False   en   \n",
       "2            650.0          6742.0  Mountain Time (US & Canada)    False   en   \n",
       "\n",
       "     screen_name                                        description  \\\n",
       "0  RobbyDelaware  I support the free movement of people, ideas a...   \n",
       "1    SCOTTGOHARD  CELEBRITY TRAINER ✨#424W147th✨ #CrossfitCoach ...   \n",
       "2    Beckster319  Rebecca Lynn Hirschfeld Actress.Model.Writer.A...   \n",
       "\n",
       "                       created_at  favourites_count  friends_count  \\\n",
       "0  Wed Jan 07 04:38:02 +0000 2009              17.0          670.0   \n",
       "1  Tue Dec 29 23:15:22 +0000 2009            2774.0         1055.0   \n",
       "2  Fri Feb 04 06:38:45 +0000 2011            7273.0          896.0   \n",
       "\n",
       "   listed_count  \n",
       "0          13.0  \n",
       "1          35.0  \n",
       "2          30.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltweets = tweets.merge(users, how='left', left_on='user_key', right_on='screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(fulltweets, 'created_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_key</th>\n",
       "      <th>created_at_x</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>...</th>\n",
       "      <th>created_strDay</th>\n",
       "      <th>created_strDayofweek</th>\n",
       "      <th>created_strDayofyear</th>\n",
       "      <th>created_strIs_month_end</th>\n",
       "      <th>created_strIs_month_start</th>\n",
       "      <th>created_strIs_quarter_end</th>\n",
       "      <th>created_strIs_quarter_start</th>\n",
       "      <th>created_strIs_year_end</th>\n",
       "      <th>created_strIs_year_start</th>\n",
       "      <th>created_strElapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.868981e+09</td>\n",
       "      <td>ryanmaxwell_1</td>\n",
       "      <td>1.458672e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>7.123460e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"IslamKills\"]</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1458671502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.571870e+09</td>\n",
       "      <td>detroitdailynew</td>\n",
       "      <td>1.476133e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>7.855849e+17</td>\n",
       "      <td>&lt;a href=\"http://twitterfeed.com\" rel=\"nofollow...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1476133020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.710805e+09</td>\n",
       "      <td>cookncooks</td>\n",
       "      <td>1.487767e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>8.343832e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1487767423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.584153e+09</td>\n",
       "      <td>queenofthewo</td>\n",
       "      <td>1.482765e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>8.134006e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"ChristmasAftermath\"]</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1482764801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.768260e+09</td>\n",
       "      <td>mrclydepratt</td>\n",
       "      <td>1.501987e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>8.940243e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1501986984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id         user_key  created_at_x  retweet_count retweeted  \\\n",
       "0  1.868981e+09    ryanmaxwell_1  1.458672e+12            NaN       NaN   \n",
       "1  2.571870e+09  detroitdailynew  1.476133e+12            0.0     False   \n",
       "2  1.710805e+09       cookncooks  1.487767e+12            NaN       NaN   \n",
       "3  2.584153e+09     queenofthewo  1.482765e+12            NaN       NaN   \n",
       "4  1.768260e+09     mrclydepratt  1.501987e+12            NaN       NaN   \n",
       "\n",
       "   favorite_count                                               text  \\\n",
       "0             NaN  #IslamKills Are you trying to say that there w...   \n",
       "1             0.0  Clinton: Trump should’ve apologized more, atta...   \n",
       "2             NaN  RT @ltapoll: Who was/is the best president of ...   \n",
       "3             NaN  RT @jww372: I don't have to guess your religio...   \n",
       "4             NaN  RT @Shareblue: Pence and his lawyers decided w...   \n",
       "\n",
       "       tweet_id                                             source  \\\n",
       "0  7.123460e+17                                                NaN   \n",
       "1  7.855849e+17  <a href=\"http://twitterfeed.com\" rel=\"nofollow...   \n",
       "2  8.343832e+17                                                NaN   \n",
       "3  8.134006e+17                                                NaN   \n",
       "4  8.940243e+17                                                NaN   \n",
       "\n",
       "                 hashtags         ...         created_strDay  \\\n",
       "0          [\"IslamKills\"]         ...                   22.0   \n",
       "1                      []         ...                   10.0   \n",
       "2                      []         ...                   22.0   \n",
       "3  [\"ChristmasAftermath\"]         ...                   26.0   \n",
       "4                      []         ...                    6.0   \n",
       "\n",
       "  created_strDayofweek created_strDayofyear  created_strIs_month_end  \\\n",
       "0                  1.0                 82.0                    False   \n",
       "1                  0.0                284.0                    False   \n",
       "2                  2.0                 53.0                    False   \n",
       "3                  0.0                361.0                    False   \n",
       "4                  6.0                218.0                    False   \n",
       "\n",
       "   created_strIs_month_start  created_strIs_quarter_end  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                      False   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "  created_strIs_quarter_start created_strIs_year_end  \\\n",
       "0                       False                  False   \n",
       "1                       False                  False   \n",
       "2                       False                  False   \n",
       "3                       False                  False   \n",
       "4                       False                  False   \n",
       "\n",
       "   created_strIs_year_start  created_strElapsed  \n",
       "0                     False          1458671502  \n",
       "1                     False          1476133020  \n",
       "2                     False          1487767423  \n",
       "3                     False          1482764801  \n",
       "4                     False          1501986984  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulltweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #IslamKills Are you trying to say that there w...\n",
       "1    Clinton: Trump should’ve apologized more, atta...\n",
       "2    RT @ltapoll: Who was/is the best president of ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulltweets.text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sentiment column\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def analyze_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "fulltweets['Sentiment'] = np.array([analyze_sentiment(str(tweet)) for tweet in fulltweets['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy implementation\n",
    "\n",
    "https://nlpforhackers.io/complete-guide-to-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA \n",
    "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "\n",
    "might be too big of data:\n",
    "\n",
    "https://turi.com/products/create/docs/graphlab.toolkits.text_analytics.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-53c7d186fa5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# generate LDA model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mldamodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    718\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                     )\n\u001b[0;32m--> 720\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0;31m# Contribution of document d to the expected sufficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0;31m# statistics for the M step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 \u001b[0msstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mphinorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = np.array([str(tweet) for tweet in fulltweets['text']]) #[doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    #stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
